Just some notes while I'm fiddling.
Started after I already have the monkey head loaded and spinning, just added the funny lighting hack.

Gemini mentioned it would look funny because we haven't added a z-buffer yet. This wasn't true. I noted
it's probably b/c there is no lighting and it's solid white rn, so I wouldn't be able to see that. It
was like "yeah, but also let me show you a cool trick to calculate normals instead of reading them
in from the obj file"

Oh....kay.......

Get derivative of the depth values in the x and y. That's the slope, and cross product gives the normal.
Makes sense when you think about it, and it's neat. You're limited by the pixel resolution kinda, and it
would probably suffer from a lack of smoothing (?).

Code replicated here as this will change:
    float3 dpdx = dfdx(in.position.xyz);
    float3 dpdy = dfdy(in.position.xyz);
    float3 normal = normalize(cross(dpdx, dpdy));

Which is neat partially b/c the dfdx, dfdy are both apparently built-ins. Same with normalize/cross but I knew that.
I'm curious if dfdx/dfdy is also automagically sampling neighbor kernels?

...

Okay added some changes for writing the depth. You use the zbuffer to cull
objects behind other objects, and I thought I'd be able to just go at it.
I wanted to do something like getting a derivative of depth for curvature highlights

Gemini pointed out "you cannot read from a texture at the same time you are writing to it"
"If you are drawing the [object] you are writing to the depth buffer. To read it for
edge detection you need Two-Pass Render".

So I can't read the depth tex while I'm drawing the object. It sounds like that's
because the depth texture is being generated at that same time actually.

Moved a copy of the old, "single pass" shader into Shaders_Single_Pass.metal, for posterity.
I'm going to dive into this shader a bit more before I move on to the 2pass.

Shaders.metal has "VertexIn", "VertexOut". VertexIn must match C++ code. VertexOut returned
by vertex_main, and accepted as arg to fragment_main, as `VertexOut in [[stage_in]]`.
Q: How does that plumbing work to connect the input of frag to the output of vert?
A: "Rasterizer". A step b/t the two shader fns.
[[stage_in]] keyword tells compiler, "This argument comes from rasterizer". Without it,
the compiler would think we're passing it from the CPU. We'd need to fill a buffer, and
have some [[buffer(n)]] where [[stage_in]] is now.

Interesting note: Gemini says the Rasterizer "blends" the values of VertexOut for all three
corner vertices, based on how close that vertex is to the pixel. This is actually somewhat how
normal smoothing works in blender! If this is true, perhaps we could calculat the normal value
in the vertex shader and pass it in, getting smoothed normals for free?

Ah, not exactly. First, dfdx/dfdy are available only fragment shaders, so not in vert_main.
Second, as I understand it, the vertices here don't know about their neighbors.
Gemini wants me to pre-calculate normals on the CPU, which seems silly. I don't plan to go
through it here, but I bet I could pass in the neighbors as a buffer and calculate a
vertex normal using these.
For dfdx, see: https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf p.214
(Also: hell yeah for that doc)

Pass 1: Rendering the object into invisible "offscreen textures" (Color + depth)
Pass 2: Render these textures into a large triangle covering the whole screen.
    -> This triangle's shader reads depth tex from pass 1, and paints to the screen.

The color and depths are passed as textures to the post process shader. Sampled with:
`
constexpr sampler s(address::clamp_to_edge, filter::linear); // See doc for things like clamp_to_edge info
float4 originalColor = colorTexture.sample(s, in.uv);
float depth = depthTexture.sample(s, in.uv).r; // 0 = near, 1.0 = far
`
sampler returns a float4. <Float/Int/etc>4s are often positions or colors and so can be indexed in two easy
ways, presumably for readability, using either .x, .y, z, .w; or .r, .g, .b, .a.
.x and .r are analogous and point to the same location.

Running a filter off the depth:
float dLeft  = depthTexture.sample(s, in.uv + float2(-offset, 0)).r;
float dRight = depthTexture.sample(s, in.uv + float2( offset, 0)).r;
float dUp    = depthTexture.sample(s, in.uv + float2(0, -offset)).r;
float dDown  = depthTexture.sample(s, in.uv + float2(0,  offset)).r;

// Simple Laplacian filter
float depthDiff = fabs(depth - dLeft) + fabs(depth - dRight) + 
                    fabs(depth - dUp)   + fabs(depth - dDown);

"Laplacian": *Divergence* of the *gradient* of a scalar function.
DIVERGENCE of the GRADIENT. So given some point p, it's the measure how much the
surrounding points are flowing away from p. Right? Double check with the robot.
* Measures the "net flow" or "flux" of gradient vectors out of point p.
* The gradient points uphill!
* If the Laplacian is positive, the *net* flow is OUTWARD from p.
    * Because gradient points uphill, that means the value of p is *LESS* than its neighbors
    * Like the bottom of a bowl
* If the Laplacian is negative, the *net* flow is INWARD towards p.
    * So like the top of a hill here.
* Laplacian = 0, net flow is even
    * Could be flat, or some other novel shapes. I think a saddle shape ought to work?

For n dimensions, the Laplacian is the sum of the derivative of each dimension.

In the draw function:
pass1->colorAttachments()->object(0)->setStoreAction(MTL::StoreActionStore); // Save for Pass 2!
MTL::StoreActionStore is what appears to cause metal to write to our designated buffer.

We additionally have:
1. pass1->colorAttachments()->object(0)->setTexture(_offscreenColorTexture); 
2. pass2->colorAttachments()->object(0)->setTexture(drawable->texture());
drawable->texture() is the screen, and _offscreenColorTexture is a texture we save for postprocess.

So along with the storeaction, we use setTexture to control the place where the texture is written.

Now we're getting to the meat of things. I see the general shader pass processes here, though.
Shader pass describes its texture outputs, then inputs, then encodes it.
So we could theoretically have a vertex pass outputting stuff like obj ids, light maps for each 
light source, etc. Then we can have a chain of post-process shaders encapsulated as objects of some
PostProcessor class. Input of each class is the path to the shader code. Then at the end we attach
a final write to screen with the output tex of the final shader in the post-processor chain.

How to attach funny vertex shaders now? I.e. wobble the vertices, or push the vertices down for an
animal crossing "cylinder world" affect.